{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39345ce-00c6-40da-a9e6-ca712413d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These were the libraies imported for use for the project.\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lib\n",
    "import IPython.display as ipd\n",
    "from pathlib import PurePosixPath, PureWindowsPath, WindowsPath\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import soundfile\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from langdetect import detect\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bdfd8f-65a7-4d58-8f83-0aeff9571ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign directory\n",
    "#This should correspond to the path where the dataset sits for reliable and accurate access\n",
    "directory = '[home_directory_path]/UGSPEECHDATA/' \n",
    "\n",
    "#This should correspond to the path where augmentation or any other additional file or folder generation takes place so as to\n",
    "#avoid unnecessary changes to dataset which can affect future use of dataset\n",
    "dir = '[any path of your choice]'\n",
    "\n",
    "# Defined column names in dataset corresponding to the excel sheet containing or transcribed audios (order doesn't matter),\n",
    "# It was reference here to be use to execute a data normalization/cleaning later on in the course of project. This was neccessary because of\n",
    "# the non-uniform naming of the columns between at least more than one of the languages selected transcribed audio file\n",
    "target_column_names = ['FILE_NO.', 'IMAGE_PATH', 'IMAGE_SRC_URL', 'AUDIO_PATH', 'TRANSCRIPTION',\n",
    "                       'SPEAKER_ID','ORG_NAME', 'PROJECT_NAME', 'LOCALE', 'GENDER', 'AGE',\n",
    "                       'DEVICE', 'ENVIRONMENT', 'YEAR','FULL_FILENAME', 'FILENAME']\n",
    "\n",
    "#This was used for data visualization purposes to label each of the langauges\n",
    "locale = ['ak_gh', 'dga_gh', 'dag_gh', 'ee_gh', 'kpo_gh'] \n",
    "\n",
    "#excel file for use\n",
    "# vital for the merging of all five langauges selected transcribed audio as one single dataframe(rown and columns datatype from the pandas library)\n",
    "target_excel = '/selected transcribed audios/selected transcribed audios.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912bb070-57c0-43b9-bca8-ec53b298cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over folder for transcriped audio file in directory\n",
    "def get_transcribed_files():\n",
    "    fileList = []\n",
    "    for name in os.listdir(directory):\n",
    "        if not name.__contains__(\".\"):\n",
    "          #\n",
    "          fileList.append(os.path.join(directory, name)+target_excel)\n",
    "            \n",
    "          #Prints directory folder names corresponding to languages \n",
    "          #print(os.path.join(name))\n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfe823-fe09-47e5-9ce6-1842ff26ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads excel data and store as dataframe\n",
    "def read_data_excel(url):\n",
    "    df = pd.read_excel(url, index_col=0, keep_default_na=False, na_values='')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923e920-6d49-40c3-b8ab-173624f0d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup column names for consolidation of languages and their coresponding data \n",
    "mapper = {}\n",
    "\n",
    "def format_column_names(dataframe):\n",
    "    for col_name in dataframe.columns:\n",
    "        for target_name in target_column_names:\n",
    "            #\n",
    "            df_column_for_compare = col_name.lower().strip().replace(' ', '_').removesuffix('2').removesuffix('s')\n",
    "            target_column_for_compare = target_name.lower().strip().replace(' ', '_').removesuffix('2').removesuffix('s')\n",
    "    \n",
    "            if df_column_for_compare == target_column_for_compare:\n",
    "                mapper[col_name] = target_name\n",
    "                break\n",
    "    \n",
    "    # Rename the columns\n",
    "    dataframe = dataframe.rename(columns=mapper)\n",
    "    \n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df8fdf-553e-44c8-85fb-84420a2e6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declares and initiates and empty list to store all consolidated languages excel data\n",
    "frame = []\n",
    "fileList = get_transcribed_files() # gets all selected transcribed audio excel file\n",
    "\n",
    "def consolidate_lang_datas():\n",
    "    \n",
    "  for file in tqdm(fileList):\n",
    "      df = read_data_excel(file) #read data from excel sheet\n",
    "      df = format_column_names(df)\n",
    "      frame.append(df) # append each new sheet as a value in the frame list\n",
    "  #    \n",
    "  print('Completed')\n",
    "\n",
    "#\n",
    "consolidate_lang_datas() # call function for the frame list to be initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284c6f8-2ca4-4088-8088-c14a1893ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns frame length\n",
    "len(frame) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd34704f-d1a7-4e47-bdf5-9f9a571d3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print fileList involved in the creation of the frame\n",
    "fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8152a6-c91c-4e41-9724-6be3e1f2c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First three entries in Frame 1   \n",
    "frame[0].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17a6a4-edfd-48d9-821d-2584d1e72bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First three entries in Frame 2\n",
    "frame[1].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a078b-fd70-4ac7-900b-8525e99e4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First three entries in Frame 3\n",
    "frame[2].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cfb4b8-ace4-494a-881b-c20a57cfce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First three entries in Frame 4\n",
    "frame[3].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f696cdb-21a7-435a-a4f2-1aaa7bd350bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First three entries in Frame 5\n",
    "frame[4].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02031200-f446-494b-b2c3-f11bd56cf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all dataframes in frame as a single dataframe for processing\n",
    "df_combined = pd.concat(frame, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30279dd8-de3c-46e0-8cd4-af36b2e2ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Structure of dataframe 1 in frame, corresponds to index 0\n",
    "frame[0].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63bc66-a1da-4f05-a863-d60ade7c4275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Structure of dataframe 4 in frame, corresponds to index 3\n",
    "frame[3].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03380e-916a-4632-bd1a-b75a8d6ff175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information on the combined dataframe containing details of selected subcribed audio of all five languages\n",
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9ee09-4c60-435c-9066-c7710b693ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process to eliminate columns which are unlikely to impact the result of training \n",
    "final_df = df_combined.drop(['FILE_NO.', 'IMAGE_SRC_URL', 'ORG_NAME', 'PROJECT_NAME', 'YEAR', 'FILENAME', 'IMAGE_PATH', \n",
    "                            'SPEAKER_ID', 'GENDER', 'AGE', 'DEVICE', 'ENVIRONMENT'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407181e8-406d-4494-b16b-af139c39a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated dataframe information after eliminating less important columns\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510dc127-0775-4fa4-aad8-cae2c0a29c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts file path from windows Path to its unix equivalent for run on unix machines\n",
    "#There won't be a need to use this function when running project on a windows machine\n",
    "def get_audio_path_unix(windowsPath):\n",
    "    path = PureWindowsPath(windowsPath)\n",
    "    audio_file = PurePosixPath(directory, *path.parts[0:])\n",
    "    audio_file.as_posix()\n",
    "    return audio_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d12efb-9f93-4333-ae92-4a0642ee3053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes files whose path seems untraceable/does not exist\n",
    "for i in tqdm(final_df.AUDIO_PATH):\n",
    "   unix_path = get_audio_path_unix(i)\n",
    "   if not os.path.exists(unix_path) and (i.startswith('Akan') or not i.startswith('Ewe') or not i.startswith('Ikposo') or not i.startswith('Dagbani') or not i.startswith('Dagaare')):\n",
    "     # print(i)\n",
    "     final_df.drop(final_df.loc[final_df['AUDIO_PATH']==i].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b77555-8de2-4cf0-a557-3a2f1f074bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final dataframe information \n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cddc14-17b3-43b7-81f9-81b2fd694ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop or delete rows who AUDIO_PATH or TRANSCRIPTION column/s equivalent data is null or None or empty\n",
    "final_df.dropna(subset=['AUDIO_PATH', 'TRANSCRIPTION'], inplace=True)\n",
    "#Gets information on dataframe after excecuting the above function\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f644c3-3bc1-45cd-8195-d7e8516ce0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets 10 entries from the final dataframe\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f16ae-0f8b-4dc6-93c7-5c3702ef411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of entries per each language in the final dataframe\n",
    "final_df.LOCALE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88541fe-008d-41a5-ad35-091a329e2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the plot of Transcription against locale in a bar chart format\n",
    "grouped_df = final_df.groupby('LOCALE').count()[['TRANSCRIPTION']]\n",
    "\n",
    "grouped_df.plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a312cf-c981-45cc-896c-9172fedc19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays number of entries corresponding to each locale on a bar chart\n",
    "def data_distribution(data):\n",
    "    plt.figure(figsize=(16,3))\n",
    "    data.LOCALE.value_counts().plot(kind='bar', title=\"Data Category distribution\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a980f9-1397-4657-b704-33468cbb5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calls the function above\n",
    "data_distribution(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401a2aa-7d90-4fa5-84d4-c462f3f755cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the first occurence of a given locale from a provided dataframe, this information is used later for data visualization purposes\n",
    "def get_first_locale_occurence(df, locale):\n",
    "    return df[df['LOCALE'] == locale].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c224b35-4e02-4042-b7de-739287808efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays audio file into its spectrogram format, which is the format that all audio files will be converted to be able to use it for training\n",
    "def spectrogram(file_path, locale):\n",
    "    y, sr = lib.load(file_path)\n",
    "    plt.figure(figsize=(16,3))\n",
    "    plt.title(locale + 'Log-Frequency Power Spectrogram')\n",
    "    data = lib.amplitude_to_db(np.abs(lib.stft(y)), ref=np.max)\n",
    "    lib.display.specshow(data, y_axis='log', x_axis='time')\n",
    "    plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a0ed2-b2af-4c9b-a0c4-d3aeb1722498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays audio file in its corresponding wave format\n",
    "def waveform(file_path, label):\n",
    "    y, sr = lib.load(file_path)\n",
    "    plt.figure(figsize=(16, 3))\n",
    "    plt.title(label + ' Sound Wave')\n",
    "    lib.display.waveshow(y, color=\"blue\")\n",
    "    # librosa.display.waveshow(y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b3004-e51c-4eb6-b4e8-6459ab51b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plays raw audio, one which can be heard\n",
    "def play_raw_audio_File(file_path):\n",
    "    return ipd.Audio(file_path) #to hear sound play in Notebooks not interactive shell like IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa969a5-c04a-421d-88a9-78e27dc2b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the spectrogram, waveform and raw audio play of the first akan language audio file in the final dataframe\n",
    "akan1_pd = get_first_locale_occurence(final_df, 'ak_gh')\n",
    "audio_path_akan = akan1_pd.iloc[0].AUDIO_PATH\n",
    "transcription_akan = akan1_pd.iloc[0].TRANSCRIPTION\n",
    "audio_unix_akan = get_audio_path_unix(audio_path_akan)\n",
    "print(audio_unix_akan)\n",
    "#\n",
    "spectrogram(audio_unix_akan, transcription_akan)\n",
    "\n",
    "\n",
    "#\n",
    "waveform(audio_unix_akan, transcription_akan)\n",
    "\n",
    "\n",
    "#\n",
    "play_raw_audio_File(audio_unix_akan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573084b-cca9-42d3-9637-df5d08c21ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the spectrogram, waveform and raw audio play of the first dagbani language audio file in the final dataframe\n",
    "dagbani1_pd = get_first_locale_occurence(final_df, 'dga_gh')\n",
    "audio_path_dagbani = dagbani1_pd.iloc[0].AUDIO_PATH\n",
    "transcription_dagbani = dagbani1_pd.iloc[0].TRANSCRIPTION\n",
    "audio_unix_dagbani = get_audio_path_unix(audio_path_dagbani)\n",
    "print(audio_unix_dagbani)\n",
    "\n",
    "#\n",
    "spectrogram(audio_unix_dagbani, transcription_dagbani)\n",
    "\n",
    "#\n",
    "waveform(audio_unix_dagbani, transcription_dagbani)\n",
    "\n",
    "#Dagbani\n",
    "play_raw_audio_File(audio_unix_dagbani)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45405c2-d803-4390-8ba3-c0b0ec89d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the spectrogram, waveform and raw audio play of the first dagaare language audio file in the final dataframe\n",
    "dagaare1_pd = get_first_locale_occurence(final_df, 'dag_gh')\n",
    "audio_path_dagaree = dagaare1_pd.iloc[0].AUDIO_PATH\n",
    "transcription_dagaare = dagaare1_pd.iloc[0].TRANSCRIPTION\n",
    "audio_unix_dagaare = get_audio_path_unix(audio_path_dagaree)\n",
    "print(audio_unix_dagaare)\n",
    "#\n",
    "spectrogram(audio_unix_dagaare, transcription_dagaare)\n",
    "\n",
    "#Dagaare\n",
    "waveform(audio_unix_dagaare, transcription_dagaare)\n",
    "\n",
    "#Dagaare\n",
    "play_raw_audio_File(audio_unix_dagaare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a425a7-1b93-48b2-b51f-63960ee25eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the spectrogram, waveform and raw audio play of the first Ewe language audio file in the final dataframe\n",
    "ewe1_pd = get_first_locale_occurence(final_df, 'ee_gh')\n",
    "audio_path_ewe = ewe1_pd.iloc[0].AUDIO_PATH\n",
    "transcription_ewe = ewe1_pd.iloc[0].TRANSCRIPTION\n",
    "audio_unix_ewe = get_audio_path_unix(audio_path_ewe)\n",
    "print(audio_unix_ewe)\n",
    "#\n",
    "spectrogram(audio_unix_ewe, transcription_ewe)\n",
    "\n",
    "#Ewe\n",
    "waveform(audio_unix_ewe, transcription_ewe)\n",
    "\n",
    "#Ewe\n",
    "play_raw_audio_File(audio_unix_ewe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce45502-c787-45df-8c24-b677ad307491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the spectrogram, waveform and raw audio play of the first ikposo language audio file in the final dataframe\n",
    "ikposo1_pd = get_first_locale_occurence(final_df, 'kpo_gh')\n",
    "audio_path_ikposo = ikposo1_pd.iloc[0].AUDIO_PATH\n",
    "transcription_ikposo = ikposo1_pd.iloc[0].TRANSCRIPTION\n",
    "audio_unix_ikposo = get_audio_path_unix(audio_path_ikposo)\n",
    "print(audio_unix_ikposo)\n",
    "#\n",
    "spectrogram(audio_unix_ikposo, transcription_ikposo)\n",
    "\n",
    "#Ikposo\n",
    "waveform(audio_unix_ikposo, transcription_ikposo)\n",
    "\n",
    "#Ikposo\n",
    "play_raw_audio_File(audio_unix_ikposo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb9f1b-403b-4146-b373-2b87e9ae314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce dataset using Locale as reference of grouping of the final dataframe. This was exceuted in order to reduce the datasize,\n",
    "# The datasize of the final_df is 93166 which happens to be too huge for my machine to process(CPU an mermory)\n",
    "df_final_sample = final_df.groupby(\"LOCALE\").sample(n=1350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee42bb-607b-4c3f-ba01-6a409d4dfe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information on the Dataframe assigned the sampling result, it has a size of 6750, which is quite smaller and manageable as opposed to the initial\n",
    "# size of 93,166\n",
    "df_final_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5ad6a-bc9d-416a-b8df-a5f2cc7e8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifies the sampling criteria of obtaining 1350 dataset from each local\n",
    "df_final_sample.LOCALE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadaed9-aecb-434a-9060-ba71632e4b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a working directory for data augmentation purposes. Data augmentation was consideered because of the imbalance in the \n",
    "# existing in the dataframe\n",
    "class distribution of the labels\n",
    "os.mkdir(dir+'working/')\n",
    "os.mkdir(dir+'working/Data2') # creates the Data2 directory where all 1350 by 5 languages == 6750 datasets or entries, in this case audio_files are transfered for augmentation purposes\n",
    "#copies files from folder into Data2, these folders in the context of our training are set_a and set_b\n",
    "def fill_folder1_toData2(): #copy files using the file path in the dataframe to the Data2 directory\n",
    "    destination = dir+'working/Data2/'\n",
    "\n",
    "    # Iterate over the files and copy them to the destination directory\n",
    "    for audio_file in tqdm(df_final_sample.AUDIO_PATH):\n",
    "        source_file = get_audio_path_unix(audio_file)\n",
    "        destination_file = os.path.join(destination, os.path.basename(source_file))\n",
    "        shutil.copy2(source_file, destination_file)\n",
    "\n",
    "fill_folder1_toData2() #call function to begin the copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d03762-c539-4221-83cc-8697ba1a0295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the OUT folder where files copied into Data2 and their augmented copy will be located\n",
    "os.mkdir(dir+'working/OUT')\n",
    "\n",
    "# get the file_path from a given folder path\n",
    "def get_fileNames(path): \n",
    "       onlyfiles = next(os.walk(path))[2] \n",
    "       return onlyfiles\n",
    "\n",
    "#returns length of files executed by get_fileNames\n",
    "len(get_fileNames(dir+'working/OUT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6800f-d227-41d6-ae5b-25272194eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitch shifting involves changing the frequency content of an audio signal while preserving its duration. This can be achieved using \n",
    "# digital signal processing techniques such as time-stretching. in the case of some audio files this form of augmentation rendered it currupted\n",
    "def changing_pitch(step, src_path, dst_path):\n",
    "    files = get_fileNames(src_path)\n",
    "    # print(len(files))\n",
    "    if not os.path.exists(dst_path):\n",
    "      os.makedirs(dst_path)\n",
    "    for file in tqdm(files):\n",
    "      filename = os.path.basename(file).replace(directory, \"\")\n",
    "      y, sr = lib.load(src_path+'/'+file)\n",
    "      updated_y = lib.effects.pitch_shift(y, sr=sr, n_steps=step)\n",
    "      soundfile.write(dst_path + '/' + filename.split('.mp3')[0] + '_' + str(step) + '.mp3', updated_y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774dcea-b558-47f3-b0ef-5826aad7306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide paramter or step for changing the pitch of the audio signal. In this case we changed the pitch by 2\n",
    "def sound_augmentation(src_path, dst_path):\n",
    "    steps = [2] #[2, -2, 2.5, -2.5]\n",
    "    for step in steps:\n",
    "        changing_pitch(step, src_path, dst_path)\n",
    "\n",
    "    files = get_fileNames(src_path)\n",
    "    for f in files:\n",
    "      shutil.copy(src_path+'/'+f, dst_path) # after changing pitch of each file, it copy all files who pitch has been changed to provided destination folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13db8e3-8b83-49a0-9167-e336a7c652f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates audio data through pitch shifting\n",
    "def create_new_augmented_data_files():\n",
    "    # Checking and creating new directory for saving newly generated audio files using data augmentation\n",
    "    if os.path.exists(dir+'working/OUT'):\n",
    "      if len(get_fileNames(dir+'working/OUT')) == 6750:\n",
    "          print('Sound Augmentation Already Done and Saved')\n",
    "      else:\n",
    "          shutil.rmtree(dir+'working/OUT')\n",
    "          sound_augmentation(dir+'working/Data2', dir+'working/OUT')\n",
    "    else:\n",
    "        sound_augmentation(dir+'working/Data2', dir+'working/OUT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37c966-0a1b-4fea-81e8-1192cdf493f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls creates create_new_augmented data files function.\n",
    "create_new_augmented_data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb3c65-4df4-4c76-9d60-284d3c61529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handles case where oringal file needs to be retrieved after augmented\n",
    "def get_aug_file(file):\n",
    "    if file.__contains__('_2.mp3'):\n",
    "        return file.split('_2.mp3')[0]+'.mp3' #other returns None type not \"\"\n",
    "        \n",
    "    elif (file.__contains__('_0.8.mp3')):\n",
    "           return file.split('_0.8.mp3')[0]+'.mp3' \n",
    "        \n",
    "    else:\n",
    "        return file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d76f9d-9273-48ae-8ddd-abb07f9b93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets transcription with audio file provided from a particular dataframe\n",
    "def get_transcription_with_audio(df, audio_file):   \n",
    "   for row in df.FULL_FILENAME:\n",
    "       if get_aug_file(row) == get_aug_file(audio_file):\n",
    "          transcription = df[df['FULL_FILENAME'] == row].TRANSCRIPTION.iloc[0]\n",
    "          return transcription\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a43df-73a9-4833-a53d-c6590b08ada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for augmented files with the help of sample files used for augmentation\n",
    "def create_dataframe(dataframe_name, folder_path):\n",
    "    new_df = {'FULL_FILENAME': [], 'TRANSCRIPTION': []}\n",
    "\n",
    "    #\n",
    "    fileNames = get_fileNames(folder_path)\n",
    "    for file in tqdm(fileNames):\n",
    "    \n",
    "        # \n",
    "        new_df['FULL_FILENAME'].append(file)\n",
    "        transcription = get_transcription_with_audio(dataframe_name, file)\n",
    "        new_df['TRANSCRIPTION'].append(transcription)\n",
    "        \n",
    "    augmented_df = pd.DataFrame(new_df)\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7159e-68ce-4e55-aab9-2a7e3050d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dataframe containg augmented files and initial audio files of 6750\n",
    "aug_df = create_dataframe(df_final_sample, dir+'working/OUT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd1186-405a-432b-abf0-e8728279aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provides information on the new created dataframe\n",
    "aug_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c83ee6-668f-4e38-917d-38557b1e2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for currupted file and remove from both dataframe and folder\n",
    "def remove_currupted_audio(folder_path, data_frame):\n",
    "    fileList = get_fileNames(folder_path)\n",
    "    cur = 0\n",
    "    not_cur = 0\n",
    "    \n",
    "    for file in tqdm(fileList):\n",
    "        try:\n",
    "            y, sr = lib.load(folder_path+file) #removed duration value of duration=3s\n",
    "            # print(file)\n",
    "            not_cur = not_cur+1\n",
    "            \n",
    "        except EOFError as e:\n",
    "           #remove from folder\n",
    "           os.remove(folder_path+file)\n",
    "\n",
    "           #get row index\n",
    "           index = data_frame[(data_frame.FULL_FILENAME == file)].index\n",
    "           #remove from dataframe\n",
    "           data_frame = data_frame.drop(index)\n",
    "            \n",
    "           cur = cur+1\n",
    "           # print(e.with_traceback)\n",
    "    return data_frame\n",
    "\n",
    "    print(cur)\n",
    "    print(not_cur)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d68427-2f16-4fe9-b720-870de0cad53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noticed a couples of files upon augmentation ended up currupted hence created a function to detect them and get rid of them\n",
    "df_aug_updated = remove_currupted_audio(dir+'working/OUT/', aug_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da98c40-7319-4457-8889-69d518a72d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Dataframe state after removing all currupted files\n",
    "df_aug_updated.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ba8ec-be62-4e84-aa14-ed48bd1a54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial dataframe state prior to augmentation\n",
    "aug_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526b8af-73ee-482f-b39a-52f73221b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort index after removal of currupted files\n",
    "df_aug_updated.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d108e0-36a7-4411-a175-be8d1c092a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare augmented dataframe with its updated version to get rid of currupted files, this is in order to obtain the files\n",
    "#that were lost in the process as they led to an un-even distribution in the class distribution in split of dataset for training and testing:\n",
    "#Error snapshot during  StratifiedShuffleSplit: 'The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.'\n",
    "df_diff = pd.merge(aug_df, df_aug_updated, how='outer', suffixes=('','_y'), indicator=True)\n",
    "rows_in_df1_not_in_df2 = df_diff[df_diff['_merge']=='left_only'][aug_df.columns]\n",
    "\n",
    "#get length of deleted currupted files\n",
    "len(rows_in_df1_not_in_df2)\n",
    "\n",
    "currupted_files_df = rows_in_df1_not_in_df2\n",
    "currupted_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b5230-52d2-4a67-bbb9-f61d6b33b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "currupted_files = currupted_files_df['FULL_FILENAME'].tolist() # Get all the list of currupted files\n",
    "currupted_files #print list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154c8fb-6798-44f8-bdfd-2f01674d0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(dir+'working/TEMP/') #creates a TEMP folder where the initial version of audio files corresponding to the currupted augmented file are stored for another appropriate form of augmentation to make dataset distribution tally or have at least more than two occuring version of a transcription\n",
    "source_folder = dir+\"working/Data2/\" \n",
    "destination_folder = dir+\"working/TEMP/\"\n",
    "\n",
    "# fetch all files from source folder corresponding to deleted currupted files into the TEMP directory in the working directory\n",
    "for file in tqdm(currupted_files):\n",
    "    # \n",
    "    file_original_name = get_aug_file(file)\n",
    "    \n",
    "    if os.path.exists(source_folder+file_original_name):\n",
    "        #\n",
    "        source = source_folder + file_original_name\n",
    "        destination = destination_folder\n",
    "\n",
    "        # copy only files\n",
    "        if os.path.isfile(source):\n",
    "            shutil.copy(source, destination)\n",
    "            print('copied', file)\n",
    "\n",
    "    else:\n",
    "        print('File doesnt exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e9aa6-944d-4bb8-a079-348d9a0382df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio file aygmented through audio signal speed change or through time strectching, it is going to be the form of augmnetation going to be administered on the files which upon pitch change became currupted\n",
    "def changing_speed(speed_rate, src_path, dst_path):\n",
    "    files = get_fileNames(src_path)\n",
    "    if not os.path.exists(dst_path):\n",
    "      os.makedirs(dst_path)\n",
    "    for file in tqdm(files):\n",
    "      filename = os.path.basename(file).replace(directory, \"\")\n",
    "      y, sr = lib.load(src_path+\"/\"+file)\n",
    "      updated_y = lib.effects.time_stretch(y, rate=speed_rate)\n",
    "      soundfile.write(dst_path + '/' + filename.split('.mp3')[0] + '_' + str(speed_rate) + \".mp3\", updated_y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d0627-e987-4849-9ad5-2c55eca56dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# administers sound augmentation by a speed rate of 0.8\n",
    "def sound_aug_without_copy(src_path, dst_path):\n",
    "    speed_rates = [0.8]\n",
    "    for speed_rate in speed_rates:\n",
    "        changing_speed(speed_rate, src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a9f4ec-1183-46f8-b940-76473b21e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(dir+'working/OUT_TEMP') #creates a folder OUT_TEMP where all newly augmented files in the TEMP folder are kept\n",
    "for file in currupted_files:\n",
    "    #original file prior to augmentation, since increasing pitch by 2 \n",
    "    #currupted audio,this time pitch reduction is going to be administed on the files\n",
    "    #pitch change to this files keep failing so an alternate augmentation procedure was administer, i.e. speed change\n",
    "    original_file = get_aug_file(file)\n",
    "\n",
    "    sound_aug_without_copy(dir+'working/TEMP', dir+'working/OUT_TEMP')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b24d76-0419-4f94-b261-5efd86757910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for currupted file after re-augmentation to ensure that re-augmentation process was successful\n",
    "def check_currupted_audio(folder_path):\n",
    "    fileList = get_fileNames(folder_path)\n",
    "    cur = 0\n",
    "    not_cur = 0\n",
    "    \n",
    "    for file in tqdm(fileList):\n",
    "        try:\n",
    "            y, sr = lib.load(folder_path+file) #removed duration value of duration=3s\n",
    "            not_cur = not_cur+1\n",
    "            \n",
    "        except EOFError as e:\n",
    "           cur = cur+1\n",
    "\n",
    "    print('Currupted Files: '+(str(cur)))\n",
    "    print('Uncurrupted Files: ' + str(not_cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c02f6-2be7-41b1-aa2b-d2345eed978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls check_currupted_audio file function\n",
    "check_currupted_audio(dir+'working/OUT_TEMP/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc3790-3844-4ca4-9d36-288916cc255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe for re-augmented currupted files\n",
    "re_aug_df = create_dataframe(currupted_files_df, dir+'working/OUT_TEMP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9206baf-c377-4786-b874-259c35710b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_aug_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa274f75-739a-48e4-97c7-504914150553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add re-augmented dataframe to the updated dataframe which was rid of the currupted files(df_aug_updated)\n",
    "aug_frames = [df_aug_updated, re_aug_df]\n",
    "final_aug_df = pd.concat(aug_frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d60491-ec41-4164-9d86-cdead837febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display re-augmnted dataframe information to verify or process taken to restore it to its initial size of 13,500\n",
    "final_aug_df.info()\n",
    "display(final_aug_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e751f7-526d-4ab4-a695-bfba7c9b055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy re-augmented currupted files into OUT folder\n",
    "\n",
    "source_folder = dir+\"working/OUT_TEMP/\"\n",
    "destination_folder = dir+\"working/OUT/\"\n",
    "\n",
    "out_temp_files = get_fileNames(source_folder)\n",
    "\n",
    "# fetch all files\n",
    "for file in tqdm(out_temp_files):\n",
    "    \n",
    "    if os.path.exists(source_folder+file):\n",
    "        #\n",
    "        source = source_folder + file\n",
    "        destination = destination_folder\n",
    "\n",
    "        # copy only files\n",
    "        if os.path.isfile(source):\n",
    "            shutil.copy(source, destination)\n",
    "            print('copied', file)\n",
    "\n",
    "    else:\n",
    "        print('File doesnt exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16adbede-f625-4dd0-9df7-52edecb4e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits final_augmented dataframe to its corresponding training and testing feature and label, 75% training data and 25% testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_aug_df['FULL_FILENAME'],final_aug_df['TRANSCRIPTION'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7e7ec-a1cd-428a-9e37-9d33593ab35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display results from training and test split\n",
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbee6d3-804c-4bf0-bd54-10474c966c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mechanisim to detect tokens in all five languages after several lang detect attempt match predicted langauges \n",
    "def tokenize_text(text): #can add language as parameter in future for languge specific token\n",
    "    # Define regex patterns for tokenization\n",
    "    patterns = [\n",
    "        r'\\b[a-zA-Z]+\\b',  # Match English words\n",
    "        r'\\b[^\\W\\d_]+\\b',  # Match any of the other five languages words (non-numeric and non-special characters)\n",
    "        r'\\b\\d+\\b'         # Match numbers\n",
    "    ]\n",
    "\n",
    "    # Combine regex patterns into a single pattern\n",
    "    combined_pattern = '|'.join(patterns)\n",
    "\n",
    "    # Tokenize text using regex\n",
    "    tokens = re.findall(combined_pattern, text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3839051-0280-4646-97fe-11b927b6abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the transcription or label\n",
    "def preprocess_transcription(transcription):\n",
    "    # Identify language or detect the language \n",
    "    try:\n",
    "        language = detect(transcription)\n",
    "    except:\n",
    "        # If language detection fails, use fallback language\n",
    "        language = 'unknown'\n",
    "    \n",
    "    # Tokenization using default tokenization function for wrongly detected languages and languagues not existing as part of langdetect classes\n",
    "    tokens = tokenize_text(transcription)\n",
    "\n",
    "    # Lowercasing, this is because speech isn't case sensitive, i.e. you can tell if part of the speech is capital or lower, so its better they\n",
    "    # are kept as lower cases\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Removing punctuation helps ensure consistency in the text or sentences by elimination nuances due to difference puuctuation styles\n",
    "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "\n",
    "\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f324455-fed1-4139-a8d0-3757dc9f2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns training data feature by obtaining melspectrogram from audio file and also pre-processing transcription\n",
    "def load_data(audio_paths, transcriptions):\n",
    "    spectrograms = []\n",
    "    processed_transcriptions = []\n",
    "\n",
    "    for audio_path, transcription in tqdm(zip(audio_paths, transcriptions)):\n",
    "        # Load audio file and compute spectrogram\n",
    "        y, sr = lib.load(dir+'working/OUT/'+audio_path, sr=None)\n",
    "        spectrogram = lib.feature.melspectrogram(y=y, sr=sr)\n",
    "        spectrogram = lib.power_to_db(spectrogram, ref=np.max)\n",
    "        spectrograms.append(spectrogram)\n",
    "\n",
    "        # Preprocess transcription\n",
    "        processed_transcription = preprocess_transcription(transcription)\n",
    "        processed_transcriptions.append(processed_transcription)\n",
    "\n",
    "    # Convert lists to NumPy arrays if needed\n",
    "    spectrogram_array = spectrogram\n",
    "    transcription_array = processed_transcriptions\n",
    "    \n",
    "    \n",
    "\n",
    "    return spectrogram_array, transcription_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95eaee9-e802-4adf-b77a-d85e225439a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and preprocess transcriptions\n",
    "audio_paths = X_train  # List of audio file paths\n",
    "transcriptions = y_train  # List of transcriptions\n",
    "spectrograms, processed_transcriptions = load_data(audio_paths, transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a714df-c436-4307-b7c9-2cb31d5af9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed transcription\n",
    "processed_transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16883d-07d4-474e-a4b1-d0bb523be3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#melspectrogram extraction of audio file\n",
    "spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def4030-b420-407c-80c4-82fdde250839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and preprocess transcriptions\n",
    "audio_paths = X_test  # List of audio file paths\n",
    "transcriptions = y_test  # List of transcriptions\n",
    "spectrograms_test, processed_transcriptions_test = load_data(audio_paths, transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21348745-299c-4208-acc4-bc9913192d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melspectrogram of audio file and preprocessed version of transcription\n",
    "spectrograms_test, processed_transcriptions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277be78-43c7-4ca1-b9d8-7ffe584219e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes max length of preprocessed transcription\n",
    "max_length = max(len(inner_list) for inner_list in processed_transcriptions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d659b-64ee-4947-bb1e-f6754aff9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min length of preprocessed transcription\n",
    "min_length = min(len(inner_list) for inner_list in processed_transcriptions)\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb994bc3-c4ba-4888-ad2a-058d3d6d66d4",
   "metadata": {},
   "source": [
    "the difference between the min and max length of the preprocessed transcription is so wide, it would have been better employing a check om audio signal initially during pre-processing stage of data to ensure audio length is capped within a given range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747991b0-0b7a-4127-b265-7ae130ede61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts train features to type numpy\n",
    "spectrograms_to_numpy = np.asarray(spectrograms, dtype=\"object\")\n",
    "x_train = spectrograms_to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c3d8a-ad6f-4a4a-8a64-5b49909ad046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts transcription or label to type numpy\n",
    "processed_transcriptions_to_numpy = np.asarray(processed_transcriptions, dtype=\"object\")\n",
    "Y_train = processed_transcriptions_to_numpy\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d9a66-1c7f-418e-91f9-c7c0634bfe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model, sets input shape to width and height due to the limiting size of the resulting numpy array\n",
    "def create_model(n_width,n_height,n_dropout,n_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(n_width,n_height)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c937dc-9f09-40ff-8463-075c56060426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_shape = x_train.shape\n",
    "num_classes = len(set(y_train))  # Number of unique tokens in transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd51420-85d2-4645-90df-0e6a27b4441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input shape of features is two dimentional\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ac958-9664-44ca-a845-6957452e4146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of classes resulting of len of unique transcription is 6,321 which is a lot, this number made it difficult to us StratifiedShuffleSplit \n",
    "# as a means of spliting as number of classes exceeded test data set if split between train and test data is capped below 50% for each\n",
    "classes = np.unique(num_classes)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5dee49-7760-415c-92ff-13a549ef2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates CNN model\n",
    "def cnn_model(x_train):\n",
    "    return create_model(x_train.shape[0], x_train.shape[1], 0.5, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f1509-9e90-440c-95e7-b9a1d3736870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attemps to create CNN model fails due to the inhomogenous shape of the x_train dataset. Increasing the dataset through additional augmentation would have fixed it to some extend as\n",
    "# as capping audio duration to say 3mins, further augmentation was going to be hard because of the limited resources of the machine used(Memory and CPU)\n",
    "cnn_model(x_train) #this cell failed or raised an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ee11ee-bf8f-440b-8f26-61b185580907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did not get the opportunity to execute this cell as a result of the preceeding cell failing.\n",
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "model.fit(padded_list_to_numpy, padded_list_to_numpy, epochs=10, validation_split=0.2)\n",
    "\n",
    "# save cnn model\n",
    "model.save(dir+'trained_model.h5')\n",
    "\n",
    "# Evaluate the  model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064dd8da-41b1-48d1-87f9-d53549135635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did not get the opportunity to execute this cell as a result of the preceeding cell failing.\n",
    "# calculate Character error rate for the model trained against the preccited transcription over the actual transcription\n",
    "def calculate_cer(predicted_transcription, ground_truth_transcription):\n",
    "    # Remove whitespace and punctuation, and convert to lowercase if necessary\n",
    "    predicted_transcription = predicted_transcription.strip().lower()\n",
    "    ground_truth_transcription = ground_truth_transcription.strip().lower()\n",
    "\n",
    "    # Calculate Character error rate\n",
    "    cer = 0\n",
    "    total_characters = max(len(predicted_transcription), len(ground_truth_transcription))\n",
    "    for p_char, g_char in zip(predicted_transcription, ground_truth_transcription):\n",
    "        if p_char != g_char:\n",
    "            cer += 1\n",
    "\n",
    "    cer /= total_characters  # Normalize by total number of characters\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747ed4a-0a4c-4819-9a6a-e7a67e785088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did not get the opportunity to execute this cell as a result of the preceeding cell failing.\n",
    "\n",
    "#loads the trained model if context is lost \n",
    "loaded_model = load_model('trained_model.h5')\n",
    "\n",
    "# Perform inference\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Evaluate predictions by calculating accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Gives the evaluation result\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e72515-e23b-489a-a049-202fd7ddb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did not get the opportunity to execute this cell as a result of the preceeding cell failing.\n",
    "#calculate perfomance on test dataset\n",
    "calculate_cer(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa2a30-00ce-4fb3-b277-0ee8a8f60b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
